---
layout: page
title: "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)"
subtitle: "reading papers"
date: 2022-09-21
categories: ["papers"]
---
    
- ## どんなものか  
- 人間に親しみやすい概念の形でニューラルネットワークの内部状態の解釈を提供する手法  
- 概念活性化ベクトル(CAV)を導入しユーザーが定義した概念が分類結果に対してどの程度重要であるかを定量化する（例えばシマウマの予測がストライプの存在にどの程度敏感であるか）  
- ![image.png](/assets/img/image_1671963078793_0.png)  
- ## 先行研究と比べて  
- 画素値(入力特徴量)の予測に対する寄与度を測る手法である Saliency maps のいくつかの手法は自然なアプローチとなる. しかし画素値レベルの説明は人間が理解しやすい上位概念に対応しないことが大きな問題である. またモデルの内部はやはり理解不能なままである.  
- そこでユーザーが定義する概念を用いてそれがどれくらい予測に影響を与えたかを示すことで解決させる.
- ## 技術や手法のポイント  
- 1. 概念の集合を定義する  
  ここで概念とは色や形状, テクスチャなどの情報のことである. ある概念を一つ定義し（例えばストライプ）その概念を表す画像例を用意する.  
- 2. Concept Actication Vectors(CAV)  
  概念の画像例集合が与えられた時, この概念を表す層lでの活性化空間における法線ベクトルを求める.  
  ![image.png](/assets/img/image_1671963146239_0.png){:height 289, :width 484}   
  これはある概念について正の集合（ストライプ模様のオブジェクトの写真）と負の集合（ランダムな写真）を用いて2値線形分類器を学習させることができ, この分類器 $v^l_C \in \mathbb{R}^m$ は概念Cに対する線形CAVである.  
- 3. 方向性微分と概念的感性  
  CAVと方向性導関数を用いることで, ある概念の方向に入力が変化した時のモデルの予測の感度を層lで測定する.   
$$
	  S_{C,k,l}(\mathbf{x}) = \lim_{\epsilon \rightarrow 0}\frac{h_{l,k}(f_l(\mathbf{x})+\epsilon v^l_C)-h_{l,k}(f_l(\mathbf{x}))}{\epsilon} = \nabla h_{l,k}(f_l(\mathbf{x})) \cdot v^l_C
	  $$ このSは任意のモデルの層における概念に対するモデル予測の感度を定量的に測定することができる.  
- 4. Testing with CAVs (TCAV)  
  CAVを用いたテストではl層のCAVが概念Cから正の影響を受けた割合としてTCAVスコアを算出する.  
$$
	  TCAV_{Q_{C,k,l}} = \frac{|\{ x \in X_k : S_{C,k,l}(x) > 0 \}|}{|X_k|}
	  $$ スコアが高ければ高いほどkクラスにおいて概念Cが重要となる.  
- ## 検証方法  
- 画像にキャプションを明示的にいれた3つのクラスのデータセットを作成する  
  ![image.png](/assets/img/image_1671963230846_0.png)  
- ここでキャプションを入れる時ある確率1-pでクラスラベル名と一致するキャプションをつけpの確率でランダムなキャプションを入れる  
- もしネットワークが分類のために画像概念を使用したのならば性能は高いままであるべきである  
- ![image.png](/assets/img/image_1671963268900_0.png)  
- 上のグラフはいろんなノイズ確率で実験を行った結果である  
- きゅうりクラスではキャプションに邪魔されて精度が下がっていることや, 実際にキャプション概念のTCAVスコアが高いことなどが確認できる  
- ## 議論