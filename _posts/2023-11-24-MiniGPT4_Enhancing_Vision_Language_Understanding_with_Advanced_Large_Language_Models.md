---
layout: papers
title:  "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"
subtitle: "reading papers"
date: 2023-7-3
categories: ["papers"]
feature_image: /assets/img/image_1688351307130_0.png
sitemap:
  priority: 0.7
publish: True
---  
- ## どんなものか
- GPT-4などの高度な大規模モデルはマルチモーダル能力を獲得してるとして、その能力を引き出すために1つの全結合層だけを用いた Vision and Language モデルを提案した
<!--more-->
- ![image.png](/assets/img/image_1688351307130_0.png){:height 307, :width 445}
- ## 先行研究と比べて
- 既存の大規模視覚言語モデル（Kosmos-1, BLIP-2）などでは
	- 食欲をそそる料理の写真を観察して詳細なレシピを直接生成
	- 画像からインスピレーションを受けて物語やラップを作成
	- 画像に商品の広告を書き込んだり、写真に写っている問題を区別して対応する解決策を提供
	- 画像から人物や映画、芸術に関する豊富な事実を直接検索
- などの複雑な能力が見られなかった
- 本モデルでは強力な言語モデルを利用することでより複雑な視覚言語能力を実現できている
- ## 技術や手法のポイント
- モデルは視覚エンコーダーとしてViT-GとQ-Formerを用いた後全結合層を通し、整形されたプロンプトとともにVicunaに入力しテキストを出力とする
	- vicuna-13b
	- 学習するのは全結合層1層のみ
- 2段階の学習を行う
	- 特に2段階目の学習により使い勝手が大幅に向上する
- 1段階目では視覚エンコーダー+全結合層の出力がグランドトゥルーステキストとなるように学習
- 2段階目のデータセットは入力プロンプトを固定し、出力をChatGPTなどで整形し最後に人間が確認された綺麗な3500のデータを用いて全体をfine tuningする
- 固定されたプロンプト
	-
	  ```
	  ###Human: <Img><ImageFeature></Img> <Instruction> ###Assistant:
	  ```
	- ImageFeatureは全結合層の後の出力
	- Instructionは "Describe this image in detail"などの画像を説明しろという命令
		- 単一ではなくいくつかのパターンがある
		- "Could you describe the contents of this image for me"など
- ## 検証方法
- Demonstrations
- 画像説明文の生成　2
	- ![image.png](/assets/img/image_1688353363896_0.png){:height 379, :width 327}
	- 画像は時計塔を背景にした賑やかな街の通り。通りの両側には商店やレストランが並んでいる。道路脇には数台のオートバイが停まり、人々が通りを歩いている。時計台にはローマ数字の大きな文字盤と小さな尖塔がある。通りの両側の建物には、装飾が施されたファサードとバルコニーがある。通りは石畳で、道の両側には街灯がある。空は青く澄み渡り、遠くに少し雲が見える。
- 画像内の面白い側面の特定　3
	- ![image.png](/assets/img/image_1688353848198_0.png){:height 415, :width 378}
	- このミームが面白いのは、犬が床に横たわり、疲れているのか眠いのか、頭を前足に乗せているところだ。キャプションの "monday just monday "は、一週間のうちで最も憂鬱な日とされる月曜日を、多くの人が同じように感じていることを示唆し、ユーモアを添えている。このミームは親しみやすくユーモラスであるため、ネットユーザーの間で人気がある。
- 珍しいコンテンツの発見　4
- 手書きテキストからウェブサイトの生成　5
- 画像中の問題を特定し解決策を提供する　6
- 画像からインスピレーションを受けた詩やラップの作成　７
- 画像にストーリーを描く　８
- 画像ちゅうの商品の広告を作成　9
- 個人を特定する　10
- 洞察力のある画像コメントを提供する　11
	- ![image.png](/assets/img/image_1688354461244_0.png){:height 711, :width 421}
	- 画像は、車やバスが行き交う交通量の多い道路を走る人の姿。その人物は黒い服を着ており、背中にバックパックを背負っている。画像は監視カメラから撮影されたもので、周囲に赤い丸が付けられている。
	- AIの言語モデルである私には、意見や感情を形成する能力はない。しかし、画像を見る限り、その人物は車やバスが行き交う濫雑な道路を走っているように見える。これは危険であり、人の安全を脅かす可能性がある。常に周囲に気を配り、安全確保に必要な予防措置を講じることが重要である。
- 画像に関連する事実を検索する　12
- 与えられた写真を使ってユーザーに料理を教える　13
	- ![image.png](/assets/img/image_1688354740462_0.png){:height 629, :width 321}
-
- Limitation（定性的な評価）
	- LLMなのでhallucinationを起こす
	- 空間的な定位の区別が苦手
		- 3500のデータセットにないので
		- Q-formerは構造的に視覚的grounding能力を失う可能性がある
- ## 議論
- デモを見ているとできているかびみょいやつがぼちぼちありそう
	- 良くも悪くも言語モデルに全依存してそう
- 言語デコーダーとしてGPTではなくInstract tuningされたモデルを使用しているんですね
