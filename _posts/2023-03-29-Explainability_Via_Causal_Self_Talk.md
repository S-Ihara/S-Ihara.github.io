---
layout: papers
title:  "Explainability Via Causal Self-Talk"
subtitle: "reading papers"
date: 2023-3-19
categories: ["papers"]
feature_image: /assets/img/image_1679208744211_0.png
sitemap:
  priority: 0.7
publish: True
---
## どんなものか
- AIシステムが自分自身の因果関係モデルを構築するように訓練することでAIシステムに対して説明可能性を付与させる手法
<!--more-->

## 先行研究と比べて
- Attribution methods
	- 入力特徴や学習例から出力への寄与を推定したり、与えられた出力に対して最適な入力を見つけることでニューラルネットの意思決定を入力レベルでモデル化しようとする方法
	- 干渉が少なく（完全にポストホックにできる）、スケーラビリティが高いことが利点としてある
	- しかし柔軟性が犠牲になり（例えば説明は全て入力表現に依存する）、解釈が非常に主観的となる
- Explicitly structuring networks
	- 中間計算が特定の形式に制約され外部データによって根拠づけられるものがある
	- 構造化中間体は設計上、根拠や解釈性になりうるが構造を明示的にするためにベースシステムを根本的に変更する必要がある
- Fine-grained mechanistic interpretability
	- ベースシステムを干渉させずに学習させ、得られた表現をポストホックに分解する方法などがある
	- 神経科学から借用、着想を得た技術を使用することが多い
	- スケーラビリティに大きな代償をもたらす
		- リバースエンジニアリングに多くの時間を費やす必要が出てきてしまう
- Decoder
	- ベースシステムの内部表現から解釈可能な表現空間へのマッピングする方法
	- デコードされた表現はベースシステムの因果経路から外れるため忠実性を犠牲にしている

## 技術や手法のポイント
- RLの問題設定で考える
	- すなわちエピソード型のPOMDPで行動するエージェントについて説明性を求めることを考える
- このとき解決策はベースシステムに組み込まれた自己モデルである
- アーキテクチャ
	- ![image.png](/assets/img/image_1679206992803_0.png)
	- CST(Causal Self-Talk)アーキテクチャでは再ルーティングされたデコーダーを拡張し、状態更新を適用する前にメモリの状態を以前の状態に戻すことも可能にする

## 検証方法
- 3D仮想環境においてCSTのバリエーションについて実験を行った
- エージェントは視覚情報とテキスト情報を入力として受け取れる
- ![image.png](/assets/img/image_1679208436502_0.png)
- One-hot-based self-talkの結果
	- ![image.png](/assets/img/image_1679208525457_0.png)
	- 左、軌跡中のメッセージの例
	- 右、CSTによりデコーダが因果関係に忠実であるかどうか
		- エラーバーは95％信頼区間を示す
		- 破線はランダムなメッセージから期待される忠実度
	-
- 言語ベースのSelf-Talk
	- ![image.png](/assets/img/image_1679208744211_0.png)
	
## 議論
-
